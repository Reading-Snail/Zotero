Poster: Rethinking Embedded Sensor Data Analysis with Large Language Models
Pramuka Sooriya Patabandige National University of Singapore pramuka@nus.edu.sg
Steven Waskito National University of Singapore steven.waskito@u.nus.edu
Kunjun Li National University of Singapore e1088150@u.nus.edu
Kai Jie Leow National University of Singapore e0959130@u.nus.edu
Shantanu Chakrabarty NCS Group shantanu.chakrabarty@ncs.com.sg
Ambuj Varshney National University of Singapore ambujv@nus.edu.sg
ABSTRACT
An important step in the deployment of wireless embedded systems is the analysis of the sensor data. Traditionally, this requires machine learning models tailored to the application use case. However, this step requires significant expertise from the end user and can be less adaptable to the dynamics of real-world deployments. In recent years, large language models have seen significant developments. These models have been shown to be capable of performing general-purpose tasks. In this work, we explore the hypothesis that large language models can be used to aid in sensor data analysis. Our preliminary findings through real-world experiments show significant promise for two tasks: inferring hand gestures through tracking of light and vibration sensor data. We believe these findings highlight the potential of large language models in sensor data analysis, and thus, it warrants further study.
CCS CONCEPTS
• Computer systems organization → Sensor networks;
KEYWORDS
Wireless embedded Systems, IoT, LLM, Sensor Data Analysis
ACM Reference Format:
Pramuka Sooriya Patabandige, Steven Waskito, Kunjun Li, Kai Jie Leow, Shantanu Chakrabarty, and Ambuj Varshney. 2023. Poster: Rethinking Embedded Sensor Data Analysis with Large Language Models. In The 21st Annual International Conference on Mobile Systems, Applications and Services (MobiSys ’23), June 18–22, 2023, Helsinki, Finland. ACM, New York, NY, USA, 2 pages. https://doi.org/10.1145/3581791.3597366
1 INTRODUCTION
Wireless embedded systems (WES) have experienced remarkable growth, with hundreds of millions of devices deployed. These devices are typically equipped with sensors, microcontrollers, wireless capabilities, and energy storage. They enable us to better understand our world by collecting data from the physical world.
Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). MobiSys ’23, June 18–22, 2023, Helsinki, Finland © 2023 Copyright held by the owner/author(s). ACM ISBN 979-8-4007-0110-8/23/06. https://doi.org/10.1145/3581791.3597366
Vibration Sensing
Light Sensing
Edge Computer GPT4 ( Large Language Model)
Figure 1: Overview. Wireless embedded systems collect sensor data from the environment. This data is transmitted to a large language model in the cloud, which analyzes it and provides insights. The results from LLM are returned to the edge device, which interprets the response for sensor data analysis.
The analysis of sensor data is critical to the deployment of WES. This step typically involves using machine learning models or algorithms to interpret the sensor data [4]. These models are often tailored explicitly for particular sensors [2] and application scenarios [1]. Using these models requires significant expertise and application domain knowledge, imposing a significant entry barrier. Recent years have seen great progress in large-scale language models (LLMs) such as ChatGPT. LLMs demonstrate impressive capabilities for completing general tasks by providing "prompts" and sensor data in natural language. Some even suggest that LLMs exhibit "sparks" of artificial general intelligence (AGI) [3]. This general intelligence may also be helpful in sensor data analysis. We present the preliminary results of our efforts to investigate the hypothesis that LLMs could assist in interpreting sensor data. The advantage of using LLMs for sensor data analysis stems from their ability to perform diverse tasks while being provided natural language inputs as prompts. This allows LLMs to handle diverse data from various application scenarios without the need to develop tailored models. Instead, imagine a situation where an end-user effortlessly inputs natural language prompts to extract insightful results and interpretations from an application deployment. We conducted a controlled experiment using a commercially available sensor platform to collect light and vibration data. We processed the sensor data with OpenAI ChatGPT (GPT-4) to infer hand gestures. Previous research has used customized machine learning models to accomplish hand gesture recognition tasks, but our early findings indicate high accuracy in identifying hand gestures even when prompting the ChatGPT system with a limited dataset. To the best of our knowledge, our investigation represents the first effort to explore the potential of large-scale language models, such as ChatGPT, for performing sensor data analysis.
561


MobiSys ’23, June 18–22, 2023, Helsinki, Finland P.Medaranga, S.Waskito, K.Li, K.J.Leow, S.Chakrabarty, and A.Varshney
1x 2x
2x 3x
Double air tap Triple air tap
1x
Single air tap
Raw Value
Time (seconds)
Single tap Double tap
Acceleration (g)
Time (seconds)
Light Readings Vibration Readings
Figure 2: Patterns in light and vibration readings due to hand gestures. Single, double, and triple tap hand gestures involve moving hand over the light sensor or tapping the table a fixed number of times.
2 DESIGN
Our system consists of WES that gather sensor data for an application and transmit their readings to an edge device. The edge device then appends relevant prompts to the sensor data and forwards this information to a LLM hosted on the cloud. The LLM analyzes the sensor data and interprets the results at the edge device. In particular, crafting appropriate prompts plays a vital role in the quality of the analysis. We present an overview of system architecture in Figure 1. In this particular work, we analyze light and vibration data to examine the LLM’s ability to recognize hand gestures. We craft our prompts first to condition the LLM with gesture data and then provide test data. We use the following prompts. Prompt (Gesture (labelled) data) for LLM: I use an RGB Light and Proximity Sensor to detect the gestures Single air tap, Double air Tap, and Triple air Tap. The sensor data collected contains the timestamp, PR and RGB; PR is proximity, and RGB indicates the Red, Green, and Blue colour values, respectively. Given below are sample data collected at three instances for each gesture:
• Baseline or No Gesture: (Light sensor readings) • Instance 1 Baseline: (Light sensor readings) • Instance 1 Single air tap: (Light sensor readings) • Instance 1 Double air tap: (Light sensor readings) Prompt (Test (unlabelled) data) for LLM: By considering the sensor data that was given initially for different gestures can you detect the gestures based on the following sensor data as no gesture, single air tap, double air tap, or triple air tap ?
• Gesture 1: (Light sensor readings) • Gesture 2: (Light sensor readings)
For conciseness, we have excluded the prompt for vibration sensor analysis; however, it is crafted similarly.
3 PRELIMINARY RESULTS
We provide early results showing the feasibility of our approach. Light sensing. We assessed the ability to detect hand gestures by analyzing changes in light intensity. When hand gestures are performed over a light sensor (APDS9960 on Nano 33 Sense), unique patterns emerge in the light readings corresponding to the gesture, as shown in Figure 2. We collected three labeled datasets (indoor light) to prompt the ChatGPT system. They consist of proximity and red, green, and blue light sensor readings. Subsequently, we conducted experiments, performing each gesture eight times, and then fed these sensor readings to ChatGPT to identify the gestures.
Figure 3: Confusion Matrix for gesture detection. We can classify hand gestures with a high accuracy despite limited prompting of LLM with labelled dataset.
Vibration sensing. We evaluated the detection of gestures based on vibrations. Tapping gestures are performed on a table. The accelerometer (LSM9DS1 IMU on Nano 33 Sense) picks up unique vibration patterns, as shown in Figure 2. Following a similar approach to earlier experiments, we prompted ChatGPT using three labelled instances. We then provided unlabelled gesture data. Results. Figure 3 presents the outcomes of our experiment. Preliminary results suggest we can identify gestures (0.95 for vibration and 0.90 for light), albeit with occasional miss classifications. However, it is essential to note that detection accuracy depends on the input prompt and the labelled data. This early-stage work involves a small study, and future investigations should incorporate a diverse set of test data to expand upon our findings.
4 CONCLUSIONS
This preliminary study analysed the capability of GPT4 in identifying a limited number of gestures based on light and vibration sensor data. The study demonstrates promising results for inferring hand gestures and detecting physical taps on a table through tracking of light and vibration sensor data. However, the study did not take into account the computation complexity and resource utilization as its intention was to primarily establish the the feasibility of using LLMs for sensor data analysis. Future studies can incorporate a wide variety of sensor data and gestures and also assess the performance of different prompts. The resulting work could be used to aid LLMs to better analyse sensor data and hence be deployed in a wide variety of applications including instances where LLMs can provide insights and organised observations based on real-time changes that take place in the real world via sensors data analysis. Acknowledgement:. This work is supported by a start-up grant and grant from NUS-NCS centre both hosted at the School of Computing of the National University of Singapore.
REFERENCES
[1] Ishan et al. Chatterjee. 2022. ClearBuds: Wireless Binaural Earbuds for LearningBased Speech Enhancement. In ACM MobiSys 2022. [2] Pete Warden et al. 2022. Machine Learning Sensors. arXiv:2206.03266 [cs.LG] [3] Sébastien Bubeck et al. 2023. Sparks of Artificial General Intelligence: Early experiments with GPT-4. arXiv:2303.12712 [cs.CL] [4] Tanmay et al. Goyal. 2023. SMiLe: Automated End-to-End Sensing and Machine Learning Co-Design. In EWSN 2022 (EWSN ’22).
562